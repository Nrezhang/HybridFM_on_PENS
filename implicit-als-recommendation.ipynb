{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8168774,"sourceType":"datasetVersion","datasetId":4169528}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\nimport re\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-15T03:40:53.445163Z","iopub.execute_input":"2025-05-15T03:40:53.445457Z","iopub.status.idle":"2025-05-15T03:40:53.825959Z","shell.execute_reply.started":"2025-05-15T03:40:53.445433Z","shell.execute_reply":"2025-05-15T03:40:53.825122Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/microsoft-pens-personalized-news-headlines/final_synthetic.csv\n/kaggle/input/microsoft-pens-personalized-news-headlines/Synthetic/Runs_Train.xlsx\n/kaggle/input/microsoft-pens-personalized-news-headlines/PENS/personalized_test.tsv\n/kaggle/input/microsoft-pens-personalized-news-headlines/PENS/train.tsv\n/kaggle/input/microsoft-pens-personalized-news-headlines/PENS/news.tsv\n/kaggle/input/microsoft-pens-personalized-news-headlines/PENS/valid.tsv\n/kaggle/input/microsoft-pens-personalized-news-headlines/personalization/pers_preprocessed.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# dataset_path = '/kaggle/input/microsoft-pens-personalized-news-headlines'\n# news_df = pd.read_csv('/kaggle/input/microsoft-pens-personalized-news-headlines/PENS/news.tsv')\n# train_df = pd.read_csv('/kaggle/input/microsoft-pens-personalized-news-headlines/PENS/train.tsv')\n# valid_df = pd.read_csv('/kaggle/input/microsoft-pens-personalized-news-headlines/PENS/valid.tsv')\ndataset_path = '/kaggle/input/microsoft-pens-personalized-news-headlines/PENS/'\n\n# Load news.tsv with tab separator and error handling\ntry:\n    news_df = pd.read_csv(dataset_path + 'news.tsv', sep='\\t', quoting=3, on_bad_lines='warn')\n    print(news_df.head())\nexcept Exception as e:\n    print(f\"Error loading file: {e}\")\n\n# Check the number of columns\nprint(f\"Number of columns: {len(news_df.columns)}\")\nprint(f\"Column names: {news_df.columns.tolist()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T03:40:53.827238Z","iopub.execute_input":"2025-05-15T03:40:53.827638Z","iopub.status.idle":"2025-05-15T03:42:14.399990Z","shell.execute_reply.started":"2025-05-15T03:40:53.827611Z","shell.execute_reply":"2025-05-15T03:42:14.399166Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_35/3279732354.py:9: ParserWarning: Skipping line 37144: expected 7 fields, saw 9\nSkipping line 64363: expected 7 fields, saw 9\n\n  news_df = pd.read_csv(dataset_path + 'news.tsv', sep='\\t', quoting=3, on_bad_lines='warn')\n","output_type":"stream"},{"name":"stdout","text":"  News ID Category         Topic  \\\n0  N10000   sports        soccer   \n1  N10001     news  newspolitics   \n2  N10002     news        newsus   \n3  N10003     news  newspolitics   \n4  N10004     news     newsworld   \n\n                                            Headline  \\\n0  Predicting Atlanta United's lineup against Col...   \n1  Mitch McConnell: DC statehood push is 'full bo...   \n2            Home In North Highlands Damaged By Fire   \n3  Meghan McCain blames 'liberal media' and 'thir...   \n4                            Today in History: Aug 1   \n\n                                           News body  \\\n0  Only FIVE internationals allowed, count em, FI...   \n1  \"WASHINGTON -- Senate Majority Leader Mitch Mc...   \n2  NORTH HIGHLANDS (CBS13)   Fire damaged a home ...   \n3  Meghan McCain is speaking out after a journali...   \n4  \"1714: George I becomes King Georg Ludwig, Ele...   \n\n                                    Title entity  \\\n0  \"{\"\"Atlanta United's\"\": 'Atlanta United FC'}\"   \n1                     {'DC': 'Washington, D.C.'}   \n2                                             {}   \n3                                             {}   \n4                                             {}   \n\n                                      Entity content  \n0  \"{'Atlanta United FC': {'type': 'item', 'id': ...  \n1  {'Washington, D.C.': {'type': 'item', 'id': 'Q...  \n2                                                 {}  \n3                                                 {}  \n4                                                 {}  \nNumber of columns: 7\nColumn names: ['News ID', 'Category', 'Topic', 'Headline', 'News body', 'Title entity', 'Entity content']\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# with open(dataset_path + 'news.tsv', 'r') as f:\n#     lines = f.readlines()\n#     for line_num in [37144, 64363]:\n#         line = lines[line_num - 1]\n#         tab_count = line.count('\\t')\n#         print(f\"Line {line_num} has {tab_count} tabs, resulting in {tab_count + 1} fields\")\n#         print(f\"Content: {line.strip()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T03:42:14.400783Z","iopub.execute_input":"2025-05-15T03:42:14.401013Z","iopub.status.idle":"2025-05-15T03:42:14.405007Z","shell.execute_reply.started":"2025-05-15T03:42:14.400991Z","shell.execute_reply":"2025-05-15T03:42:14.404194Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"train_df = pd.read_csv(dataset_path + 'train.tsv', sep='\\t', quoting=3, on_bad_lines='warn')\ntrain_df = train_df.head(100000)\nprint(len(train_df))\nprint(train_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T03:42:14.406908Z","iopub.execute_input":"2025-05-15T03:42:14.407155Z","iopub.status.idle":"2025-05-15T03:42:39.661881Z","shell.execute_reply.started":"2025-05-15T03:42:14.407134Z","shell.execute_reply":"2025-05-15T03:42:39.660594Z"}},"outputs":[{"name":"stdout","text":"100000\n    UserID                                        ClicknewsID  \\\n0  U335175  N41340 N27570 N83288 N100653 N105490 N88971 N5...   \n1  U146053  N95202 N84319 N43563 N78688 N94858 N60541 N109...   \n2  U158889  N104156 N103864 N51305 N109347 N70019 N106791 ...   \n3   U22232  N25386 N35729 N32113 N102243 N62352 N22227 N10...   \n4   U32515  N55509 N15992 N27093 N78610 N13691 N103166 N11...   \n\n                                           dwelltime  \\\n0  116 23 59 146 661 303 59 199 165 144 110 29 32...   \n1  105 62 41 11 52 300 55 17 73 52 145 24 96 38 3...   \n2                             175 58 2 871 169 69 10   \n3  4 15 3 31 9 26 216 208 58 7 481 32 5 29 13 2 1...   \n4  47 123 54 166 29 56 10 131 298 1592 146 48 45 ...   \n\n                                       exposure_time  \\\n0  6/19/2019 5:10:01 AM#TAB#6/19/2019 5:11:58 AM#...   \n1  6/14/2019 11:54:18 AM#TAB#6/14/2019 11:57:02 A...   \n2  6/17/2019 1:23:34 PM#TAB#6/17/2019 1:27:47 PM#...   \n3  6/15/2019 10:42:34 AM#TAB#6/15/2019 10:43:35 A...   \n4  6/14/2019 3:15:04 PM#TAB#6/14/2019 3:16:31 PM#...   \n\n                                   pos  \\\n0  N55476 N103556 N52756 N58377 N98303   \n1                 N77413 N88971 N65878   \n2                        N72751 N62314   \n3                        N31619 N98303   \n4                        N78883 N56068   \n\n                                                 neg                  start  \\\n0  N48119 N92507 N92467 N35759 N16584 N56727 N449...    7/3/2019 6:43:49 AM   \n1  N92854 N40725 N39488 N76001 N82971 N74955 N345...  6/23/2019 10:28:02 AM   \n2  N84182 N72110 N122127 N24095 N96945 N26759 N34...   6/28/2019 8:50:00 AM   \n3  N90820 N44438 N43527 N55476 N23227 N49161 N564...    7/3/2019 2:27:50 PM   \n4  N111634 N19030 N122006 N32123 N50002 N42107 N2...   6/26/2019 9:20:16 AM   \n\n                     end    dwelltime_pos  \n0    7/3/2019 7:06:06 AM  34 83 79 234 16  \n1  6/23/2019 10:54:50 AM          5 34 90  \n2   6/28/2019 9:00:09 AM          246 137  \n3    7/3/2019 2:31:44 PM             20 8  \n4   6/26/2019 9:55:27 AM          105 279  \n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"valid_df = pd.read_csv(dataset_path + 'valid.tsv', sep='\\t', quoting=3, on_bad_lines='warn')\n# valid_df.head(1000)\nprint(valid_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T03:42:39.663414Z","iopub.execute_input":"2025-05-15T03:42:39.663881Z","iopub.status.idle":"2025-05-15T03:42:49.894189Z","shell.execute_reply.started":"2025-05-15T03:42:39.663835Z","shell.execute_reply":"2025-05-15T03:42:49.893308Z"}},"outputs":[{"name":"stdout","text":"    UserID                                        ClicknewsID  \\\n0  U180794  N27499 N49500 N76930 N121334 N117744 N95202 N3...   \n1  U311381  N96591 N54516 N39614 N84898 N102774 N10817 N88...   \n2   U84844  N90215 N109222 N42583 N59544 N54907 N28628 N99...   \n3  U174959  N21530 N14869 N43119 N94858 N91626 N36905 N881...   \n4   U56319  N78421 N59628 N43119 N94858 N47462 N49906 N115...   \n\n                                           dwelltime  \\\n0  59 701 12 73 83 29 1 9 10 15 41 89 18 7 54 163...   \n1            133 125 63 13 163 36 77 1207 590 422 70   \n2  345 633 521 52 39 116 168 149 159 210 78 17 27...   \n3  3 105 28 91 89 6 9 164 605 4 53 284 38 29 10 1...   \n4  8 798 706 1186 38 36 54 234 149 71 143 298 121...   \n\n                                       exposure_time  \\\n0  6/14/2019 6:00:54 AM#TAB#6/14/2019 6:01:54 AM#...   \n1  6/17/2019 9:14:48 AM#TAB#6/19/2019 9:07:18 AM#...   \n2  6/17/2019 9:19:53 AM#TAB#6/17/2019 1:31:45 PM#...   \n3  6/14/2019 3:09:00 PM#TAB#6/14/2019 3:09:12 PM#...   \n4  6/13/2019 8:50:30 PM#TAB#6/14/2019 3:57:13 AM#...   \n\n                                                 pos  \\\n0                       N42089 N61006 N16638 N101047   \n1                                      N42089 N71890   \n2  N85070 N105901 N19244 N99713 N98450 N73639 N40...   \n3                                             N72458   \n4                                             N73460   \n\n                                                 neg                 start  \\\n0  N117616 N82971 N88969 N104339 N67941 N107765 N...   7/9/2019 9:46:51 AM   \n1  N24296 N113124 N87357 N111496 N92768 N74772 N7...   7/9/2019 2:27:22 PM   \n2  N106268 N97313 N74897 N90052 N91007 N13909 N16...  7/12/2019 7:02:43 AM   \n3  N115143 N74300 N120574 N108795 N94325 N72797 N...  7/7/2019 11:28:37 PM   \n4  N119100 N29839 N52126 N42813 N83195 N24694 N19...  7/9/2019 10:30:12 PM   \n\n                    end                       dwelltime_pos  \n0   7/9/2019 9:59:14 AM                        347 24 98 39  \n1   7/9/2019 3:14:39 PM                            1476 103  \n2  7/12/2019 7:49:22 AM  687 127 115 53 308 218 43 57 28 82  \n3  7/7/2019 11:36:02 PM                                  59  \n4  7/9/2019 10:40:09 PM                                 101  \n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"print(\"Train DataFrame:\")\nprint(\"Train columns:\", train_df.columns.tolist())\nprint(\"\\nValidation DataFrame:\")\nprint(\"Valid columns:\", valid_df.columns.tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T03:42:49.895355Z","iopub.execute_input":"2025-05-15T03:42:49.895687Z","iopub.status.idle":"2025-05-15T03:42:49.901304Z","shell.execute_reply.started":"2025-05-15T03:42:49.895664Z","shell.execute_reply":"2025-05-15T03:42:49.900486Z"}},"outputs":[{"name":"stdout","text":"Train DataFrame:\nTrain columns: ['UserID', 'ClicknewsID', 'dwelltime', 'exposure_time', 'pos', 'neg', 'start', 'end', 'dwelltime_pos']\n\nValidation DataFrame:\nValid columns: ['UserID', 'ClicknewsID', 'dwelltime', 'exposure_time', 'pos', 'neg', 'start', 'end', 'dwelltime_pos']\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"print(\"Sample of train.tsv:\")\nprint(train_df[['UserID', 'ClicknewsID', 'exposure_time', 'start', 'end']].head())\nprint(\"\\nTimestamp ranges:\")\nprint(f\"exposure_time: {train_df['exposure_time'].min()} to {train_df['exposure_time'].max()}\")\nprint(f\"start: {train_df['start'].min()} to {train_df['start'].max()}\")\nprint(f\"end: {train_df['end'].min()} to {train_df['end'].max()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T03:42:49.902415Z","iopub.execute_input":"2025-05-15T03:42:49.902735Z","iopub.status.idle":"2025-05-15T03:42:50.033196Z","shell.execute_reply.started":"2025-05-15T03:42:49.902700Z","shell.execute_reply":"2025-05-15T03:42:50.032223Z"}},"outputs":[{"name":"stdout","text":"Sample of train.tsv:\n    UserID                                        ClicknewsID  \\\n0  U335175  N41340 N27570 N83288 N100653 N105490 N88971 N5...   \n1  U146053  N95202 N84319 N43563 N78688 N94858 N60541 N109...   \n2  U158889  N104156 N103864 N51305 N109347 N70019 N106791 ...   \n3   U22232  N25386 N35729 N32113 N102243 N62352 N22227 N10...   \n4   U32515  N55509 N15992 N27093 N78610 N13691 N103166 N11...   \n\n                                       exposure_time                  start  \\\n0  6/19/2019 5:10:01 AM#TAB#6/19/2019 5:11:58 AM#...    7/3/2019 6:43:49 AM   \n1  6/14/2019 11:54:18 AM#TAB#6/14/2019 11:57:02 A...  6/23/2019 10:28:02 AM   \n2  6/17/2019 1:23:34 PM#TAB#6/17/2019 1:27:47 PM#...   6/28/2019 8:50:00 AM   \n3  6/15/2019 10:42:34 AM#TAB#6/15/2019 10:43:35 A...    7/3/2019 2:27:50 PM   \n4  6/14/2019 3:15:04 PM#TAB#6/14/2019 3:16:31 PM#...   6/26/2019 9:20:16 AM   \n\n                     end  \n0    7/3/2019 7:06:06 AM  \n1  6/23/2019 10:54:50 AM  \n2   6/28/2019 9:00:09 AM  \n3    7/3/2019 2:31:44 PM  \n4   6/26/2019 9:55:27 AM  \n\nTimestamp ranges:\nexposure_time: 6/13/2019 10:00:08 PM#TAB#6/13/2019 10:03:27 PM#TAB#6/13/2019 10:06:37 PM#TAB#6/13/2019 10:29:30 PM#TAB#6/13/2019 10:29:53 PM#TAB#6/13/2019 10:39:18 PM#TAB#6/13/2019 11:16:22 PM#TAB#6/14/2019 8:43:06 AM#TAB#6/14/2019 8:44:27 AM#TAB#6/14/2019 8:45:37 AM#TAB#6/14/2019 8:46:07 AM#TAB#6/14/2019 8:50:25 AM#TAB#6/14/2019 8:50:29 AM#TAB#6/14/2019 5:10:32 PM#TAB#6/15/2019 8:52:49 AM#TAB#6/15/2019 10:54:31 AM#TAB#6/15/2019 11:56:33 AM to 7/3/2019 9:31:07 PM\nstart: 6/14/2019 10:02:02 PM to 7/4/2019 9:59:59 AM\nend: 6/14/2019 10:00:05 PM to 7/4/2019 9:59:58 AM\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"Splitting training set into train and validation using a Time-based Split","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom tqdm import tqdm\n\ndef temporal_split(data, split_time=None):\n    data['first_exposure'] = data['exposure_time'].apply(\n        lambda x: pd.to_datetime(x.split('#TAB#')[0], format='%m/%d/%Y %I:%M:%S %p', errors='coerce') if pd.notna(x) else pd.NaT\n    )\n    \n    if split_time is None:\n        split_time = data['first_exposure'].quantile(0.8)\n    else:\n        split_time = pd.to_datetime(split_time)\n    \n    print(f\"Split time: {split_time}\")\n    \n    train_rows = []\n    valid_rows = []\n    skipped_rows = 0\n    \n    for _, row in tqdm(data.iterrows(), total=len(data)):\n        user_id = row['UserID']\n        try:\n            exposure_times = row['exposure_time'].split('#TAB#')\n            exposure_times = [pd.to_datetime(t, format='%m/%d/%Y %I:%M:%S %p', errors='coerce') for t in exposure_times]\n        except:\n            skipped_rows += 1\n            continue\n        \n        click_ids = row['ClicknewsID'].split() if pd.notna(row['ClicknewsID']) else []\n        pos_ids = row['pos'].split() if pd.notna(row['pos']) else []\n        neg_ids = row['neg'].split() if pd.notna(row['neg']) else []\n        \n        for cid, t in zip(click_ids, exposure_times[:len(click_ids)]):\n            if pd.isna(t):\n                skipped_rows += 1\n                continue\n            row_data = (user_id, cid, t, 1)\n            if t < split_time:\n                train_rows.append(row_data)\n            else:\n                valid_rows.append(row_data)\n        \n        t = exposure_times[0] if exposure_times else pd.NaT\n        if not pd.isna(t):\n            for pid in pos_ids:\n                row_data = (user_id, pid, t, 1)\n                if t < split_time:\n                    train_rows.append(row_data)\n                else:\n                    valid_rows.append(row_data)\n        \n        for nid in neg_ids:\n            if pd.isna(t):\n                skipped_rows += 1\n                continue\n            row_data = (user_id, nid, t, 0)\n            if t < split_time:\n                train_rows.append(row_data)\n            else:\n                valid_rows.append(row_data)\n    \n    print(f\"Skipped {skipped_rows} rows due to invalid timestamps\")\n    \n    columns = ['UserID', 'NewsID', 'exposure_time', 'Clicked']\n    train_interactions_df = pd.DataFrame(train_rows, columns=columns)\n    valid_interactions_df = pd.DataFrame(valid_rows, columns=columns)\n    \n    train_interactions_df = train_interactions_df.drop_duplicates(subset=['UserID', 'NewsID'])\n    valid_interactions_df = valid_interactions_df.drop_duplicates(subset=['UserID', 'NewsID'])\n    \n    train_interactions_df.to_csv('/kaggle/working/train_split.csv', index=False)\n    valid_interactions_df.to_csv('/kaggle/working/valid_split.csv', index=False)\n    \n    print(f\"Validation interactions: {len(train_interactions_df)}\")\n    print(f\"Test interactions: {len(valid_interactions_df)}\")\n    print(f\"Validation positive: {len(train_interactions_df[train_interactions_df['Clicked'] == 1])}\")\n    print(f\"Test positive: {len(valid_interactions_df[valid_interactions_df['Clicked'] == 1])}\")\n    print(f\"Validation negative: {len(train_interactions_df[train_interactions_df['Clicked'] == 0])}\")\n    print(f\"Test negative: {len(valid_interactions_df[valid_interactions_df['Clicked'] == 0])}\")\n    return train_interactions_df, valid_interactions_df\n\n\ntrain_interactions_df, valid_interactions_df = temporal_split(valid_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T03:42:50.034221Z","iopub.execute_input":"2025-05-15T03:42:50.034435Z","iopub.status.idle":"2025-05-15T04:07:31.647670Z","shell.execute_reply.started":"2025-05-15T03:42:50.034416Z","shell.execute_reply":"2025-05-15T04:07:31.646757Z"}},"outputs":[{"name":"stdout","text":"Split time: 2019-06-17 09:20:16.800000\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100000/100000 [23:10<00:00, 71.89it/s] \n","output_type":"stream"},{"name":"stdout","text":"Skipped 0 rows due to invalid timestamps\nValidation interactions: 3696091\nTest interactions: 10762218\nValidation positive: 1367150\nTest positive: 10185762\nValidation negative: 2328941\nTest negative: 576456\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# def extract_interactions(df):\n#     interactions = []\n#     for _, row in tqdm(df.iterrows(), total=len(df)):\n#         user_id = row['UserID']\n#         click_ids = row['ClicknewsID'].split() if pd.notna(row['ClicknewsID']) else []\n#         for cid in click_ids:\n#             interactions.append((user_id, cid, 1))\n#         if pd.notna(row['pos']):\n#             for news_id in row['pos'].split():\n#                 interactions.append((user_id, news_id, 1))\n#         if pd.notna(row['neg']):\n#             for news_id in row['neg'].split():\n#                 interactions.append((user_id, news_id, 0))\n#     return pd.DataFrame(interactions, columns=['UserID', 'NewsID', 'Clicked'])\n\n# train_df = pd.read_csv(dataset_path + 'train.tsv', sep='\\t', quoting=3, on_bad_lines='warn', nrows=400000)\n# train_interactions_df = extract_interactions(train_df)\n# train_interactions_df = train_interactions_df.drop_duplicates(subset=['UserID', 'NewsID'])\n# train_interactions_df.to_csv('/kaggle/working/train_interactions.csv', index=False)\n\n# print(f\"Training interactions: {len(train_interactions_df)}\")\n# print(f\"Training positive: {len(train_interactions_df[train_interactions_df['Clicked'] == 1])}\")\n# print(f\"Training negative: {len(train_interactions_df[train_interactions_df['Clicked'] == 0])}\")\n# print(\"Training sample:\")\n# print(train_interactions_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T04:07:31.648980Z","iopub.execute_input":"2025-05-15T04:07:31.649325Z","iopub.status.idle":"2025-05-15T04:07:31.654276Z","shell.execute_reply.started":"2025-05-15T04:07:31.649293Z","shell.execute_reply":"2025-05-15T04:07:31.653464Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"from scipy.sparse import csr_matrix\nimport numpy as np\n\nuser_ids = train_interactions_df['UserID'].unique()\nnews_ids = train_interactions_df['NewsID'].unique()\nuser2idx = {uid: idx for idx, uid in enumerate(user_ids)}\nnews2idx = {nid: idx for idx, nid in enumerate(news_ids)}\n\nrows, cols, data = [], [], []\nfor _, row in train_interactions_df.iterrows():\n    rows.append(user2idx[row['UserID']])\n    cols.append(news2idx[row['NewsID']])\n    data.append(row['Clicked'])\n\ninteraction_matrix = csr_matrix((data, (rows, cols)), shape=(len(user_ids), len(news_ids)))\nprint(f\"Interaction matrix shape: {interaction_matrix.shape}\")\nprint(f\"Number of non-zero entries: {interaction_matrix.nnz}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T04:07:31.657091Z","iopub.execute_input":"2025-05-15T04:07:31.657361Z","iopub.status.idle":"2025-05-15T04:10:24.496650Z","shell.execute_reply.started":"2025-05-15T04:07:31.657339Z","shell.execute_reply":"2025-05-15T04:10:24.495659Z"}},"outputs":[{"name":"stdout","text":"Interaction matrix shape: (77924, 20271)\nNumber of non-zero entries: 3696091\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"!pip install implicit","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T04:10:24.497711Z","iopub.execute_input":"2025-05-15T04:10:24.498345Z","iopub.status.idle":"2025-05-15T04:10:30.462490Z","shell.execute_reply.started":"2025-05-15T04:10:24.498320Z","shell.execute_reply":"2025-05-15T04:10:30.461366Z"}},"outputs":[{"name":"stdout","text":"Collecting implicit\n  Downloading implicit-0.7.2-cp311-cp311-manylinux2014_x86_64.whl.metadata (6.1 kB)\nRequirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from implicit) (1.26.4)\nRequirement already satisfied: scipy>=0.16 in /usr/local/lib/python3.11/dist-packages (from implicit) (1.15.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from implicit) (4.67.1)\nRequirement already satisfied: threadpoolctl in /usr/local/lib/python3.11/dist-packages (from implicit) (3.6.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.0->implicit) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.0->implicit) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.0->implicit) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.0->implicit) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.0->implicit) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.0->implicit) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.0->implicit) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.0->implicit) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17.0->implicit) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17.0->implicit) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17.0->implicit) (2024.2.0)\nDownloading implicit-0.7.2-cp311-cp311-manylinux2014_x86_64.whl (8.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: implicit\nSuccessfully installed implicit-0.7.2\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"from implicit.als import AlternatingLeastSquares\n\n# Train ALS model\nmodel = AlternatingLeastSquares(factors=50, iterations=15, regularization=0.1)\nmodel.fit(interaction_matrix)  # Users as rows\n\ndef recommend_items(user_id, interaction_matrix, model, user2idx, news2idx, top_n=10):\n    if user_id not in user2idx:\n        return []\n    user_idx = user2idx[user_id]\n    user_items = interaction_matrix[user_idx:user_idx+1]\n    rec_indices, _ = model.recommend(user_idx, user_items, N=top_n, filter_already_liked_items=True)\n    top_news_ids = [list(news2idx.keys())[list(news2idx.values()).index(idx)] for idx in rec_indices]\n    return top_news_ids\n\nsample_user = train_interactions_df['UserID'].iloc[0]\nrecommendations = recommend_items(sample_user, interaction_matrix, model, user2idx, news2idx)\nprint(f\"Recommendations for user {sample_user}: {recommendations}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T04:10:30.463925Z","iopub.execute_input":"2025-05-15T04:10:30.464266Z","iopub.status.idle":"2025-05-15T04:10:52.300483Z","shell.execute_reply.started":"2025-05-15T04:10:30.464222Z","shell.execute_reply":"2025-05-15T04:10:52.299184Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/implicit/cpu/als.py:95: RuntimeWarning: Intel MKL BLAS is configured to use 2 threads. It is highly recommended to disable its internal threadpool by setting the environment variable 'MKL_NUM_THREADS=1' or by callng 'threadpoolctl.threadpool_limits(1, \"blas\")'. Having MKL use a threadpool can lead to severe performance issues\n  check_blas_config()\n/usr/local/lib/python3.11/dist-packages/implicit/cpu/als.py:95: RuntimeWarning: OpenBLAS is configured to use 4 threads. It is highly recommended to disable its internal threadpool by setting the environment variable 'OPENBLAS_NUM_THREADS=1' or by calling 'threadpoolctl.threadpool_limits(1, \"blas\")'. Having OpenBLAS use a threadpool can lead to severe performance issues here.\n  check_blas_config()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/15 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f3bde3733cb477b8bb8c899d95b1a45"}},"metadata":{}},{"name":"stdout","text":"Recommendations for user U180794: ['N121349', 'N39862', 'N81795', 'N75910', 'N122875', 'N31171', 'N55111', 'N51924', 'N119816', 'N116986']\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import numpy as np\nfrom tqdm.notebook import tqdm\n\ndef evaluate_recommendations(eval_df, interaction_matrix, model_or_similarity, user2idx, news2idx, top_n=10, use_implicit=False, sample_frac=1.0):\n    recall_sum, precision_sum, ndcg_sum, mrr_sum, total = 0, 0, 0, 0, 0\n    \n    # Get unique users and sample if sample_frac < 1.0\n    unique_users = eval_df['UserID'].unique()\n    if sample_frac < 1.0:\n        unique_users = np.random.choice(unique_users, size=int(len(unique_users) * sample_frac), replace=False)\n        print(f\"Evaluating on {len(unique_users)} sampled users ({sample_frac*100:.1f}% of {len(eval_df['UserID'].unique())} total users)\")\n    \n    # Evaluate with tqdm progress bar\n    for user_id in tqdm(unique_users, desc=\"Evaluating users\", leave=True):\n        if user_id not in user2idx:\n            continue\n        # Get ground truth (positive interactions)\n        ground_truth = set(eval_df[(eval_df['UserID'] == user_id) & (eval_df['Clicked'] == 1)]['NewsID'].tolist())\n        if not ground_truth:\n            continue\n            \n        # Get recommendations\n        if use_implicit:\n            recs = recommend_items(user_id, interaction_matrix, model_or_similarity, user2idx, news2idx, top_n)\n        else:\n            recs = recommend_items(user_id, interaction_matrix, model_or_similarity, news2idx, top_n)\n        recs = set(recs)\n        \n        # Relevant items in recommendations\n        relevant = len(recs & ground_truth)\n        \n        # Recall@K: Proportion of relevant items retrieved\n        recall = relevant / len(ground_truth) if len(ground_truth) > 0 else 0\n        recall_sum += recall\n        \n        # Precision@K: Proportion of recommended items that are relevant\n        precision = relevant / len(recs) if len(recs) > 0 else 0\n        precision_sum += precision\n        \n        # NDCG@K\n        dcg = 0\n        for i, item in enumerate(recs, 1):\n            if item in ground_truth:\n                dcg += 1 / np.log2(i + 1)\n        idcg = sum(1 / np.log2(i + 1) for i in range(1, min(len(ground_truth), top_n) + 1))\n        ndcg = dcg / idcg if idcg > 0 else 0\n        ndcg_sum += ndcg\n        \n        # MRR: Reciprocal of the rank of the first relevant item\n        for rank, item in enumerate(recs, 1):\n            if item in ground_truth:\n                mrr_sum += 1 / rank\n                break\n        \n        total += 1\n    \n    recall_k = recall_sum / total if total > 0 else 0\n    precision_k = precision_sum / total if total > 0 else 0\n    ndcg_k = ndcg_sum / total if total > 0 else 0\n    mrr = mrr_sum / total if total > 0 else 0\n    \n    return recall_k, precision_k, ndcg_k, mrr\n\n# Evaluate on validation set (25% sample)\ntry:\n    # Using implicit.als\n    recall_valid, precision_valid, ndcg_valid, mrr_valid = evaluate_recommendations(\n        valid_interactions_df, interaction_matrix, model, user2idx, news2idx, use_implicit=True, sample_frac=0.01\n    )\nexcept:\n    # Fallback to cosine_similarity\n    recall_valid, precision_valid, ndcg_valid, mrr_valid = evaluate_recommendations(\n        valid_interactions_df, interaction_matrix, item_similarity, user2idx, news2idx, sample_frac=0.25\n    )\nprint(f\"Validation Recall@10: {recall_valid:.4f}, Precision@10: {precision_valid:.4f}, NDCG@10: {ndcg_valid:.4f}, MRR: {mrr_valid:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T04:19:06.726839Z","iopub.execute_input":"2025-05-15T04:19:06.727177Z","iopub.status.idle":"2025-05-15T04:30:04.859327Z","shell.execute_reply.started":"2025-05-15T04:19:06.727152Z","shell.execute_reply":"2025-05-15T04:30:04.858254Z"}},"outputs":[{"name":"stdout","text":"Evaluating on 974 sampled users (1.0% of 97437 total users)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating users:   0%|          | 0/974 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa9c82a4ece34377a3402a9a2b0c6328"}},"metadata":{}},{"name":"stdout","text":"Validation Recall@10: 0.0011, Precision@10: 0.0124, NDCG@10: 0.0144, MRR: 0.0453\n","output_type":"stream"}],"execution_count":16}]}